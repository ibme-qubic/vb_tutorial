{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational Bayes\n",
    "==============\n",
    "\n",
    "This notebook implements the example from section 3 of the FMRIB tutorial on Variational Bayes (\"Inferring a single Gaussian\").\n",
    "\n",
    "We assume we have data drawn from a Gaussian distribution with true mean $\\mu$ and true precision $\\beta$:\n",
    "\n",
    "$$\n",
    "P(y_n | \\mu, \\beta) = \\frac{\\sqrt{\\beta}}{\\sqrt{2\\pi}} \\exp{-\\frac{\\beta}{2} (y_n - \\mu)^2}\n",
    "$$\n",
    "\n",
    "One interpretation of this is that our data consists of repeated measurements of a fixed value ($\\mu$) combined with Gaussian noise with standard deviation $\\frac{1}{\\sqrt{\\beta}}$.\n",
    "\n",
    "Here's how we can generate some sample data from this model in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data samples are:\n",
      "[41.91506205 41.71575743 41.21722831 41.1370187  42.20863143 42.59805747\n",
      " 42.35461184 42.2284233  41.62441638 40.52181549 43.35424082 43.20326743\n",
      " 40.6623724  41.0719153  43.22959433 43.2517624  39.8364154  41.6555438\n",
      " 43.3878903  39.85646884 42.30221124 42.68612582 41.93744701 42.46153288\n",
      " 44.39718024 41.49607225 42.10620187 41.49204145 43.47033085 42.35962569\n",
      " 40.2925708  43.14713237 43.67537866 41.93484999 41.35153808 40.93458194\n",
      " 41.39637645 42.5793365  40.8740474  41.6990064  41.98543867 43.97378619\n",
      " 42.39505716 43.24613681 41.97429464 42.49155949 39.95958648 42.14421519\n",
      " 42.6970443  42.56907244 43.83837602 43.45420334 41.26660935 43.33753292\n",
      " 43.78267009 41.6649501  40.51958079 42.85492039 42.19767491 41.24531766\n",
      " 43.158234   41.75973371 42.77644856 42.62072848 41.88296869 43.19522357\n",
      " 43.214627   42.72940096 41.51869356 42.55731413 43.1652834  43.27025683\n",
      " 40.13810988 43.75861594 43.13565448 42.14293123 41.4934171  41.45517698\n",
      " 42.40900417 40.88036431 41.44973401 42.94166146 42.33757275 42.24823803\n",
      " 41.70579076 41.45159223 41.65481532 42.27461538 41.77440416 41.62571471\n",
      " 40.37655159 42.53559522 41.55528297 41.5031091  41.86314301 41.39586047\n",
      " 43.26158875 43.19031625 42.01326844 41.55690919]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Ground truth parameters\n",
    "# We infer the precision, BETA, but it is useful to\n",
    "# derive the variance and standard deviation from it\n",
    "MU_TRUTH = 42\n",
    "BETA_TRUTH = 1.0\n",
    "VAR_TRUTH = 1/BETA_TRUTH\n",
    "STD_TRUTH = np.sqrt(VAR_TRUTH)\n",
    "\n",
    "# Observed data samples are generated by Numpy from the ground truth\n",
    "# Gaussian distribution. Reducing the number of samples should make\n",
    "# the inference less 'confident' - i.e. the output variances for\n",
    "# MU and BETA will increase\n",
    "N = 100\n",
    "DATA = np.random.normal(MU_TRUTH, STD_TRUTH, [N])\n",
    "print(\"Data samples are:\")\n",
    "print(DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Variational Bayes we make the approximation that the posterior is factorised with respect to these two parameters:\n",
    "\n",
    "$$q(\\mu, \\beta) = q(\\mu)q(\\beta)$$\n",
    "\n",
    "Generally it is a requirement for the analytic formulation of Variational Bayes that the 'noise' and 'signal' parameters are factorised. In more complex examples where there is more than one 'signal' parameter (e.g. we are inferring the parameters of a complex nonlinear model), a combined distribution may be used for the multiple signal parameters, however the noise must still be factorised.\n",
    "\n",
    "Another requirement for analytic VB is the choice of 'conjugate' distributions for the priors $P(\\mu)$, $P(\\beta)$, and the posteriors $q(\\mu)$ and $q(\\beta)$. This arises from Bayes's theorem which in our approximate form is:\n",
    "\n",
    "$$q(\\mu)q(\\beta) \\propto P(\\textbf{Y} | \\mu, \\beta)P(\\mu)P(\\beta)$$\n",
    "\n",
    "Here, $P(\\textbf{Y} | \\mu, \\beta)$ is the likelihood and is determined from the Gaussian data model given above. It turns out that if we choose certain types of distribution for the priors $P(\\mu)$ and $P(\\beta)$, then $q(\\mu)$ and $q(\\beta)$ will end up having the same type of distribution. These 'special' distributions are known as the 'conjugate' distributions *for the likelihood*. Conjugate distributions depend on the exact form of the likelihood function.\n",
    "\n",
    "We will not prove the conjugate distributions for this likelihood, but will simply state that for a Gaussian data model as above, the conjugate distribution for $\\mu$ is Gaussian, the the conjugate distribution for $\\beta$ is a Gamma distribution:\n",
    "\n",
    "$$P(\\mu) = \\frac{1}{\\sqrt{2\\pi v_0}} \\exp{-\\frac{1}{2v_0}(\\mu - m_0)^2}$$\n",
    "$$q(\\mu) = \\frac{1}{\\sqrt{2\\pi v}} \\exp{-\\frac{1}{2v}(\\mu - m)^2}$$\n",
    "\n",
    "Here $m$ and $v$ are the 'hyperparameters' of the posterior for $\\mu$ - they determine the inferred posterior distribution of \n",
    "$\\mu$, and through the VB formulation we will infer values for them from the data. $m_0$ and $v_0$ similarly describe our prior knowledge of the likely value of $\\mu$ and might incorporate existing knowledge. Alternatively by choosing a large value of $v_0$ we can have a 'non-informative' prior which would be used if we have no real idea before seeing the data what the value of $\\mu$ might be.\n",
    "\n",
    "Similarly for $\\beta$ we have:\n",
    "\n",
    "$$P(\\beta) = \\frac{1}{\\Gamma(c_0)}\\frac{\\beta^{c_0-1}}{b_0^{c_0}}\\exp{-\\frac{\\beta}{b_0}}$$\n",
    "$$q(\\beta) = \\frac{1}{\\Gamma(c)}\\frac{\\beta^{c-1}}{b^c}\\exp{-\\frac{\\beta}{b}}$$\n",
    "\n",
    "Here $b$ and $c$ are the inferred hyperparameters and $b_0$ and $c_0$ are the prior scale and shape parameters for the Gamma distribution. The mean of the Gamma distribution is given by $cb$ and the variance by $cb^2$. \n",
    "\n",
    "Sometimes it may be more intuitive to think of the Gamma prior in terms of a mean and variance in which case we can derive the prior hyperparameters as:\n",
    "\n",
    "$$b_0 = \\frac{\\textrm{Prior variance}}{\\textrm{Prior mean}}$$\n",
    "$$c_0 = \\frac{\\textrm{Prior mean}^2}{\\textrm{Prior variance}}$$\n",
    "\n",
    "Here we'll define some non-informative priors for $\\mu$ and $\\beta$. Note that for the noise prior, $\\beta$ we define the prior mean and variance and derive the prior hyperparameters $b_0$ and $c_0$ from this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Priors: P(mu) = N(0.000000, 1000.000000), P(beta) = Ga(1000.000000, 0.001000)\n"
     ]
    }
   ],
   "source": [
    "m0 = 0\n",
    "v0 = 1000\n",
    "beta_mean0 = 1\n",
    "beta_var0 = 1000\n",
    "b0 = beta_var0 / beta_mean0\n",
    "c0 = beta_mean0**2 / beta_var0\n",
    "print(\"Priors: P(mu) = N(%f, %f), P(beta) = Ga(%f, %f)\" % (m0, v0, b0, c0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need the update equations. These will take existing values of $m$, $v$, $b$ and $c$ and produce new estimates. By repeatedly iterating we will converge on the optimal posterior hyperparameters.\n",
    "\n",
    "The update equations are derived by applying the Calculus of Variations to the problem of maximising the free energy - see the tutorial sections 2.2, 3.1 and 3.2 for the derivation.\n",
    "\n",
    "Here we implement the update equations as a Python function which takes values of $m$, $v$, $b$ and $c$ and returns updated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation 3.15 - these depend only on the data\n",
    "S1 = np.sum(DATA)\n",
    "S2 = np.sum(np.square(DATA))\n",
    "\n",
    "def update(m, v, b, c):\n",
    "    # Equation 3.17\n",
    "    m = (m0 + v0 * b * c * S1) / (1 + N * v0 * b * c)\n",
    "\n",
    "    # Equation 3.18\n",
    "    v = v0 / (1 + N * v0 * b * c)\n",
    "\n",
    "    # Equation 3.20\n",
    "    X = S2 - 2*S1*m + N * (m**2 + v)\n",
    "\n",
    "    # Equation 3.21\n",
    "    b = 1 / (1 / b0 + X / 2)\n",
    "    \n",
    "    # Equation 3.22\n",
    "    c = N / 2 + c0\n",
    "    \n",
    "    return m, v, b, c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterative process needs some starting values which we define similarly to the priors. We could in fact start off with the prior values. If the iterative process is working the starting values should not matter, however in more complex problems it is important to start out with reasonable values of the parameters or the iteration can become stuck in a local maximum and not find the best solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial values: (m, v, b, c) = (0.000000, 10.000000, 10.000000, 0.100000)\n"
     ]
    }
   ],
   "source": [
    "m = 0\n",
    "v = 10\n",
    "beta_mean1 = 1.0\n",
    "beta_var1 = 10\n",
    "b = beta_var1 / beta_mean1\n",
    "c = beta_mean1**2 / beta_var1\n",
    "print(\"Initial values: (m, v, b, c) = (%f, %f, %f, %f)\" % (m, v, b, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our initial values are not particularly close to the true values, so we are not cheating!\n",
    "\n",
    "Finally, let's iterate 10 times and see what happens to the hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: (m, v, b, c) = (42.131579, 0.010000, 0.020469, 50.001000)\n",
      "Iteration 2: (m, v, b, c) = (42.131589, 0.009771, 0.020473, 50.001000)\n",
      "Iteration 3: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 4: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 5: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 6: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 7: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 8: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 9: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Iteration 10: (m, v, b, c) = (42.131589, 0.009768, 0.020474, 50.001000)\n",
      "Inferred mean/precision of Gaussian: 42.131589, 1.023696\n",
      "Inferred variance on Gaussian mean/precision: 0.009768, 0.020959\n"
     ]
    }
   ],
   "source": [
    "for vb_iter in range(10):\n",
    "    m, v, b, c = update(m, v, b, c)\n",
    "    print(\"Iteration %i: (m, v, b, c) = (%f, %f, %f, %f)\" % (vb_iter+1, m, v, b, c))\n",
    "\n",
    "print(\"Inferred mean/precision of Gaussian: %f, %f\" % (m, c * b))\n",
    "print(\"Inferred variance on Gaussian mean/precision: %f, %f\" % (v, c * b**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update equations in this case converge within just a couple of iterations and lead to inferred hyperparameters close to our ground truth. Note also that we have inferred the variance on these parameters - this gives an indication of how confident we can be in their values. If you try reducing the number of samples in the data set the variance will increase since we have less information to infer $\\mu$ and $\\beta$.\n",
    "\n",
    "Other things to try would include:\n",
    "\n",
    " - Reducing the variance of the priors, i.e. make them informative. This will cause the inferred values to move closer to the\n",
    "   prior values because we are now claiming we have prior knowledge of what $\\mu$ and $\\beta$ must be, and this can to some\n",
    "   extent override the information in the data.\n",
    "   \n",
    " - Changing the initial values of $m$, $v$, $b$ and $c$ to verify that the iteration can still converge to the correct    \n",
    "   solution.\n",
    " \n",
    " - Try modifying the ground truth values and verify that we still infer the correct solution.\n",
    " \n",
    " - Increasing the level of noise should cause the variance estimates in the parameters to go up\n",
    " - Reducing the number of data samples should also increase the variance estimates to go up since we have less information to go on.\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
