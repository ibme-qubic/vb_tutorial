#!/usr/bin/env python

import numpy as np
import tensorflow as tf

# Ground truth parameters
# We infer the precision, BETA, but it is useful to
# derive the variance and standard deviation from it
MU_TRUTH = 42
BETA_TRUTH = 1.0
VAR_TRUTH = 1/BETA_TRUTH
STD_TRUTH = np.sqrt(VAR_TRUTH)

# Observed data samples are generated by Numpy from the ground truth
# Gaussian distribution. Reducing the number of samples should make
# the inference less 'confident' - i.e. the output variances for
# MU and BETA will increase
N = 100
DATA = np.random.normal(MU_TRUTH, STD_TRUTH, [N])
print("Data samples are:")
print(DATA)

m0 = 0.0
v0 = 100000.0
b0 = 0.0
w0 = 100000.0
print("Priors: P(mu, log(1/beta)) = MVN([%f, %f], [[%f, 0], [0, %f]])" % (m0, v0, b0, w0))

data = tf.constant(DATA, dtype=tf.float32)
prior_means = tf.constant([m0, v0], dtype=tf.float32)
prior_covariance = tf.constant([[v0, 0.0], [0.0, w0]], dtype=tf.float32)

post_means_init = [0.0, 0.0]
post_covariance_init = [[1.0, 0.0], [0.0, 1.0]]

chol_off_diag = tf.Variable([[0, 0], [0, 0]], dtype=tf.float32)
chol_log_diag = tf.Variable(tf.log(tf.diag_part(post_covariance_init)), dtype=tf.float32)
chol_diag = tf.diag(tf.sqrt(tf.exp(chol_log_diag)))
post_covariance_chol = tf.add(chol_diag, tf.matrix_band_part(chol_off_diag, -1, 0))

post_covariance = tf.matmul(tf.transpose(post_covariance_chol), post_covariance_chol)
post_means = tf.Variable(post_means_init, dtype=tf.float32)
  

sess = tf.Session()
sess.run(tf.initialize_all_variables())
print("Initial posterior mean: %s" % sess.run(post_means))
print("Initial posterior covariance:\n%s" % sess.run(post_covariance))

# Number of samples from the posterior
S=5

# eps is a sample from a Gaussian with mean 0 and variance 1
eps = tf.random_normal((2, S), 0, 1, dtype=tf.float32)

# Start off each sample with the current posterior mean
# post_samples is now a tensor of shape [2, n_samples]
samples = tf.tile(tf.reshape(post_means, [2, 1]), [1, S])

# Now add the random sample scaled by the covariance
post_samples = tf.add(samples, tf.matmul(post_covariance_chol, eps))

print(sess.run(post_samples))

# These are out sample of values for the mu parameter
mu_samples = post_samples[0]

# Get the current estimate of the noise variance 1/beta remembering that
# we are inferring the log of the noise variance
log_noise = post_samples[1]
noise_var = tf.exp(log_noise)

# Each sample value predicts the full set of values in the data sample.
# For our constant-signal model, the prediction is simply a set of 
# constant values. The prediction tensor will have shape [S, N]
# where S is the sample size and N is the number of data values
prediction = tf.tile(tf.reshape(mu_samples, [S, 1]), [1, N])

# To calculate the likelihood we need the sum of the squared difference between the data  
# and the prediction. This gives a value for each posterior sample so has shape [S]
sum_square_diff = tf.reduce_sum(tf.square(data - prediction), axis=-1)

# Now we calculate the likelihood for each posterior sample (shape [S])
# Note that we are ignoring constant factors such as 2*PI here as they 
# are just an fixed offset and do not affect the optimization 
log_likelihood = 0.5 * (log_noise * tf.to_float(N) + sum_square_diff / noise_var)

# Finally to evaluate the expectation value we take the mean across all the posterior
# samples
reconstr_loss = tf.reduce_mean(log_likelihood)

print("Reconstruction loss is: %f" % sess.run(reconstr_loss))

C = post_covariance
C0 = prior_covariance
C0_inv = tf.matrix_inverse(C0)

# m - m0 as row and column vectors
m_minus_m0 = tf.reshape(tf.subtract(post_means, prior_means), [-1, 1])
m_minus_m0_T = tf.reshape(tf.subtract(post_means, prior_means), [1, -1])

term1 = tf.trace(tf.matmul(C0_inv, C))
term2 = -tf.log(tf.matrix_determinant(C) / tf.matrix_determinant(C0))

# Size of the MVN distribution
term3 = -2
term4 = tf.matmul(tf.matmul(m_minus_m0_T, C0_inv), m_minus_m0)
          
latent_loss = 0.5 * (term1 + term2 + term3 + term4)

print("Latent loss is: %f" % sess.run(latent_loss))

cost = reconstr_loss + latent_loss
print("Total cost is: %f" % sess.run(cost))

optimizer = tf.train.AdamOptimizer(learning_rate=0.5)
minimizer = optimizer.minimize(cost)
sess.run(tf.global_variables_initializer())

cost_history = []
for epoch in range(5000):
    sess.run(minimizer)
    cost_history.append(float(sess.run(cost)))
    print("Epoch %i: cost=%f, posterior means=%s" % (epoch+1, sess.run(cost), sess.run(post_means)))

final_means = sess.run(post_means)
final_covariance = sess.run(post_covariance)
print("Estimate for mu: %f (variance: %f)" % (final_means[0], final_covariance[0, 0]))
print("Estimate for beta: %f" % np.exp(-final_means[1]))

from matplotlib import pyplot as plt
plt.plot(cost_history)
plt.ylim(50000, 51000)
plt.show()